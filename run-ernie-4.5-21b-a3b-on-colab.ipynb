{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PFCCLab/ERNIE4.5-Developer-Resource/blob/main/run-ernie-4.5-21b-a3b-on-colab.ipynb)\n",
        "\n",
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!"
      ],
      "metadata": {
        "id": "JxMAhhWtxNTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build llama.cpp\n",
        "Let's first build the latest llama.cpp from source, which support ERNIE 4.5 MoE LLMs. You'll need to wait for a while to let it finish."
      ],
      "metadata": {
        "id": "UgcV9iy2y3Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp\n",
        "!cd llama.cpp && cmake -B build -DGGML_CUDA=ON && cmake --build build --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22ac6x18WskA",
        "outputId": "7d1d61d1-df29-4c0d-b004-df8b74be900b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- CUDA Toolkit found\n",
            "-- Using CUDA architectures: native\n",
            "-- CUDA host compiler is GNU 11.4.0\n",
            "-- Including CUDA backend\n",
            "-- ggml version: 0.0.5967\n",
            "-- ggml commit:  6c88b3bb\n",
            "-- Configuring done (0.4s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  2%] Built target ggml-base\n",
            "[ 30%] Built target ggml-cuda\n",
            "[ 35%] Built target ggml-cpu\n",
            "[ 36%] Built target ggml\n",
            "[ 44%] Built target llama\n",
            "[ 44%] Built target build_info\n",
            "[ 48%] Built target common\n",
            "[ 49%] Built target test-tokenizer-0\n",
            "[ 50%] Built target test-sampling\n",
            "[ 51%] Built target test-grammar-parser\n",
            "[ 52%] Built target test-grammar-integration\n",
            "[ 53%] Built target test-llama-grammar\n",
            "[ 54%] Built target test-chat\n",
            "[ 55%] Built target test-json-schema-to-grammar\n",
            "[ 56%] Built target test-quantize-stats\n",
            "[ 56%] Built target test-gbnf-validator\n",
            "[ 57%] Built target test-tokenizer-1-bpe\n",
            "[ 57%] Built target test-tokenizer-1-spm\n",
            "[ 58%] Built target test-chat-parser\n",
            "[ 59%] Built target test-chat-template\n",
            "[ 60%] Built target test-json-partial\n",
            "[ 61%] Built target test-log\n",
            "[ 62%] Built target test-regex-partial\n",
            "[ 62%] Built target test-thread-safety\n",
            "[ 63%] Built target test-arg-parser\n",
            "[ 64%] Built target test-gguf\n",
            "[ 65%] Built target test-backend-ops\n",
            "[ 66%] Built target test-model-load-cancel\n",
            "[ 67%] Built target test-autorelease\n",
            "[ 68%] Built target test-barrier\n",
            "[ 69%] Built target test-quantize-fns\n",
            "[ 70%] Built target test-quantize-perf\n",
            "[ 71%] Built target test-rope\n",
            "[ 72%] Built target mtmd\n",
            "[ 72%] Built target test-mtmd-c-api\n",
            "[ 72%] Built target test-c\n",
            "[ 72%] Built target llama-batched\n",
            "[ 72%] Built target llama-embedding\n",
            "[ 73%] Built target llama-eval-callback\n",
            "[ 73%] Built target sha256\n",
            "[ 74%] Built target xxhash\n",
            "[ 75%] Built target sha1\n",
            "[ 75%] Built target llama-gguf-hash\n",
            "[ 76%] Built target llama-gguf\n",
            "[ 77%] Built target llama-gritlm\n",
            "[ 77%] Built target llama-lookahead\n",
            "[ 78%] Built target llama-lookup\n",
            "[ 79%] Built target llama-lookup-create\n",
            "[ 79%] Built target llama-lookup-merge\n",
            "[ 80%] Built target llama-lookup-stats\n",
            "[ 81%] Built target llama-parallel\n",
            "[ 81%] Built target llama-passkey\n",
            "[ 81%] Built target llama-retrieval\n",
            "[ 82%] Built target llama-save-load-state\n",
            "[ 82%] Built target llama-simple\n",
            "[ 83%] Built target llama-simple-chat\n",
            "[ 84%] Built target llama-speculative\n",
            "[ 84%] Built target llama-speculative-simple\n",
            "[ 84%] Built target llama-gen-docs\n",
            "[ 84%] Built target llama-finetune\n",
            "[ 85%] Built target llama-diffusion-cli\n",
            "[ 86%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 87%] Built target llama-vdot\n",
            "[ 87%] Built target llama-q8dot\n",
            "[ 88%] Built target llama-batched-bench\n",
            "[ 89%] Built target llama-gguf-split\n",
            "[ 89%] Built target llama-imatrix\n",
            "[ 89%] Built target llama-bench\n",
            "[ 90%] Built target llama-cli\n",
            "[ 91%] Built target llama-perplexity\n",
            "[ 92%] Built target llama-quantize\n",
            "[ 93%] Built target llama-server\n",
            "[ 94%] Built target llama-run\n",
            "[ 95%] Built target llama-tokenize\n",
            "[ 95%] Built target llama-tts\n",
            "[ 96%] Built target llama-llava-cli\n",
            "[ 97%] Built target llama-gemma3-cli\n",
            "[ 97%] Built target llama-minicpmv-cli\n",
            "[ 98%] Built target llama-qwen2vl-cli\n",
            "[ 99%] Built target llama-mtmd-cli\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model\n",
        "\n",
        "The model we are going to use is https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF\n",
        "\n",
        "We will use `huggingface-cli` to download a 4-bit quatalized ERNIE-4.5-21B-A3B model. You will need to wait for a while to let the model being downloaded.\n"
      ],
      "metadata": {
        "id": "_RjXc86NDpQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHMdaPBWlBxK",
        "outputId": "fc378619-d3ef-4874-e1a3-d8b83460c782"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (1.1.5)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.51)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.7.14)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download unsloth/ERNIE-4.5-21B-A3B-PT-GGUF ERNIE-4.5-21B-A3B-PT-Q4_0.gguf --local-dir ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm7elMoylfHN",
        "outputId": "e16640fa-4553-410b-b9aa-4ea7f5eff61c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'ERNIE-4.5-21B-A3B-PT-Q4_0.gguf' to '.cache/huggingface/download/qf9UfM-59aZ2UT6kBnX_9enaNpU=.584935fb4723b3c2fd208c76ffb7512f544f518de90302290d971b0f06e5fad1.incomplete'\n",
            "ERNIE-4.5-21B-A3B-PT-Q4_0.gguf: 100% 12.4G/12.4G [07:34<00:00, 27.3MB/s]\n",
            "Download complete. Moving file to ERNIE-4.5-21B-A3B-PT-Q4_0.gguf\n",
            "ERNIE-4.5-21B-A3B-PT-Q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model\n",
        "We will use `llama-cli` to run this model, the `--jinja` argument is required as discssed in https://github.com/ggml-org/llama.cpp/pull/14658#issuecomment-3082745420."
      ],
      "metadata": {
        "id": "npmbOEtE1M3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!!llama.cpp/build/bin/llama-cli -m ./ERNIE-4.5-21B-A3B-PT-Q4_0.gguf --jinja -p \"tell me a joke\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_5nRiH9l5hQ",
        "outputId": "925e4491-abd3-428e-9c2f-452a496decff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "build: 5967 (6c88b3bb) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 45 key-value pairs and 389 tensors from ./ERNIE-4.5-21B-A3B-PT-Q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = ernie4_5-moe\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Ernie-4.5-21B-A3B-Pt\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = PT\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Ernie-4.5-21B-A3B-Pt\n",
            "llama_model_loader: - kv   5:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 21B-A3B\n",
            "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
            "llama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   9:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv  10:                  general.base_model.0.name str              = ERNIE 4.5 21B A3B PT\n",
            "llama_model_loader: - kv  11:          general.base_model.0.organization str              = Baidu\n",
            "llama_model_loader: - kv  12:              general.base_model.0.repo_url str              = https://huggingface.co/baidu/ERNIE-4....\n",
            "llama_model_loader: - kv  13:                               general.tags arr[str,3]       = [\"ERNIE4.5\", \"unsloth\", \"text-generat...\n",
            "llama_model_loader: - kv  14:                          general.languages arr[str,2]       = [\"en\", \"zh\"]\n",
            "llama_model_loader: - kv  15:                   ernie4_5-moe.block_count u32              = 28\n",
            "llama_model_loader: - kv  16:                ernie4_5-moe.context_length u32              = 131072\n",
            "llama_model_loader: - kv  17:              ernie4_5-moe.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv  18:           ernie4_5-moe.feed_forward_length u32              = 12288\n",
            "llama_model_loader: - kv  19:          ernie4_5-moe.attention.head_count u32              = 20\n",
            "llama_model_loader: - kv  20:       ernie4_5-moe.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  21:                ernie4_5-moe.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  22: ernie4_5-moe.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  23:                  ernie4_5-moe.expert_count u32              = 64\n",
            "llama_model_loader: - kv  24:             ernie4_5-moe.expert_used_count u32              = 6\n",
            "llama_model_loader: - kv  25:     ernie4_5-moe.interleave_moe_layer_step u32              = 1\n",
            "llama_model_loader: - kv  26:     ernie4_5-moe.leading_dense_block_count u32              = 1\n",
            "llama_model_loader: - kv  27:    ernie4_5-moe.expert_feed_forward_length u32              = 1536\n",
            "llama_model_loader: - kv  28:           ernie4_5-moe.expert_shared_count u32              = 2\n",
            "llama_model_loader: - kv  29: ernie4_5-moe.expert_shared_feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv  30:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  31:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  32:                      tokenizer.ggml.tokens arr[str,103424]  = [\"<unk>\", \"<s>\", \"</s>\", \"0\", \"1\", \"2...\n",
            "llama_model_loader: - kv  33:                      tokenizer.ggml.scores arr[f32,103424]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  34:                  tokenizer.ggml.token_type arr[i32,103424]  = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  35:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  36:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  37:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  38:                    tokenizer.chat_template str              = {%- if not add_generation_prompt is d...\n",
            "llama_model_loader: - kv  39:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  40:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  41:                      quantize.imatrix.file str              = ERNIE-4.5-21B-A3B-PT-GGUF/imatrix_uns...\n",
            "llama_model_loader: - kv  42:                   quantize.imatrix.dataset str              = unsloth_calibration_ERNIE-4.5-21B-A3B...\n",
            "llama_model_loader: - kv  43:             quantize.imatrix.entries_count u32              = 304\n",
            "llama_model_loader: - kv  44:              quantize.imatrix.chunks_count u32              = 622\n",
            "llama_model_loader: - type  f32:  111 tensors\n",
            "llama_model_loader: - type q4_0:  272 tensors\n",
            "llama_model_loader: - type q4_1:    5 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 11.54 GiB (4.54 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 1015\n",
            "load: token to piece cache size = 0.5907 MB\n",
            "print_info: arch             = ernie4_5-moe\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 20\n",
            "print_info: n_head_kv        = 4\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 5\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 12288\n",
            "print_info: n_expert         = 64\n",
            "print_info: n_expert_used    = 6\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 21B.A3B\n",
            "print_info: model params     = 21.83 B\n",
            "print_info: general.name     = Ernie-4.5-21B-A3B-Pt\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 103424\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 0 '<unk>'\n",
            "print_info: LF token         = 23 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 0 repeating layers to GPU\n",
            "load_tensors: offloaded 0/29 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size = 11820.97 MiB\n",
            ".........................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.39 MiB\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB\n",
            "llama_kv_cache_unified: size =  224.00 MiB (  4096 cells,  28 layers,  1/ 1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context:      CUDA0 compute buffer size =   424.13 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    13.01 MiB\n",
            "llama_context: graph nodes  = 1825\n",
            "llama_context: graph splits = 447 (with bs=512), 1 (with bs=1)\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n",
            "main: chat template example:\n",
            "<|begin_of_sentence|>You are a helpful assistant\n",
            "User: Hello\n",
            "Assistant: Hi there<|end_of_sentence|>User: How are you?\n",
            "Assistant: \n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 3671767489\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            " User: tell me a joke\n",
            "Assistant: 为什么数学书总是很忧郁？  \n",
            "因为它有太多的问题（problems）！\n",
            "\n",
            "> "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "Ok, that's it, have fun~"
      ],
      "metadata": {
        "id": "t7rhmE3z13ZU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4PUvhio17Zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}